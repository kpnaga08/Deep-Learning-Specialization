{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='magenta'>Kristina P. Sinaga.</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 \n",
    "\n",
    "What is the \"cache\" used for in our implementation of forward propagation and backward propagation?\n",
    "\n",
    "- **<font color='yellow'>[x] We use it to pass variables computed during forward propagation to the corresponding backward propagation step. It contains useful values for backward propagation to compute derivatives.</font>**\n",
    "\n",
    "- [ ] It is used to cache the intermediate values of the cost function during training.\n",
    "\n",
    "- [ ] We use it to pass $Z$ computed during backward propagation to the corresponding forward propagation step. It contains useful values for forward propagation to compute activations.\n",
    "\n",
    "- [ ] It is used to keep track of the hyperparameters that we are searching over, to speed up computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "Among the following, which ones are \"hyperparameters\"? (Check all that apply.) \n",
    "\n",
    "- **<font color='yellow'>[x] number of iterations</font>** \n",
    "\n",
    "- **<font color='yellow'>[x] size of the hidden layers $n^{[l]}$</font>** \n",
    "\n",
    "- [ ] activation value $a^{[1]}$\n",
    "\n",
    "- **<font color='yellow'>[x] number of layers $L$ in the neural network</font>** \n",
    "\n",
    "- [ ] bias vector $b^{[1]}$\n",
    "\n",
    "- [ ] weight matrices $W^{[1]}$\n",
    "    \n",
    "- **<font color='yellow'>[x] learning rate $\\alpha$</font>** \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "Which of the following statements is true? \n",
    "\n",
    "- The earlier layers of a neural network are typically computing more complex features of the input than the deeper layers.\n",
    "\n",
    "- **<font color='yellow'>[x] The deeper layers of a neural network are typically computing more complex features of the input than the earlier layers.</font>** \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "\n",
    "Vectorization allows you to compute forward propagation in an $L$-layer neural network without an explicit for-loop (or any other explicit iterative loop) over the layers $l=1, 2, …,L$. True/False?\n",
    "\n",
    "- **<font color='yellow'>[x] False</font>** \n",
    "  \n",
    "  **<font color='green'> Forward propagation propagates the input through the layers, although for shallow networks we may just write all the lines ($a^{[2]} = g^{[2]}(z^{[2]})$, $z^{[2]}= W^{[2]}a^{[1]}+b^{[2]}$, ...) in a deeper network, we cannot avoid a for loop iterating over the layers: ($a^{[l]} = g^{[l]}(z^{[l]})$, $z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$, ...).</font>** \n",
    "\n",
    "  \n",
    "- True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "\n",
    "Assume we store the values for $n^{[l]}$ in an array called layers, as follows: $layer_dims = [n_x, 4,3,2,1]$. So layer 1 has four hidden units, layer 2 has 3 hidden units and so on. Which of the following for-loops will allow you to initialize the parameters for the model?\n",
    "\n",
    "- \n",
    "    ```\n",
    "    for(i in range(1, len(layer_dims))):\n",
    "        parameter[‘W’ + str(i)] = np.random.randn(layer_dims[i-1], layer_dims[i])) * 0.01\n",
    "        parameter[‘b’ + str(i)] = np.random.randn(layer_dims[i], 1) * 0.01\n",
    "\n",
    "    ```   \n",
    "\n",
    "- \n",
    "    ```\n",
    "    for i in range(1, len(layer_dims)/2)):\n",
    "        parameter[‘W’ + str(i)] = np.random.randn(layer_dims[i], layer_dims[i-1])) * 0.01\n",
    "        parameter[‘b’ + str(i)] = np.random.randn(layer_dims[i-1], 1) * 0.01\n",
    "\n",
    "    ```      \n",
    "\n",
    "- **<font color='yellow'>[x]</font>**\n",
    "    ```\n",
    "    for(i in range(1, len(layer_dims))):\n",
    "        parameter[‘W’ + str(i)] = np.random.randn(layers[i], layers[i - 1])) * 0.01\n",
    "        parameter[‘b’ + str(i)] = np.random.randn(layers[i], 1) * 0.01\n",
    "    ```\n",
    "\n",
    "\n",
    "- \n",
    "    ```\n",
    "    for i in range(1, len(layer_dims)/2)):\n",
    "        parameter[‘W’ + str(i)] = np.random.randn(layer_dims[i], layer_dims[i-1])) * 0.01\n",
    "        parameter[‘b’ + str(i)] = np.random.randn(layer_dims[i], 1) * 0.01\n",
    "\n",
    "    ```   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6\n",
    "\n",
    "Consider the following neural network:\n",
    "\n",
    "<figure>\n",
    " <img src=\"./W4.png\"   style=\"width:800px;height:400px;\">\n",
    "</figure>\n",
    "\n",
    "What are all the values of $n^{[0]}, n^{[1]}, n^{[2]}, n^{[3]},$ and $n^{[4]}$ ?\n",
    "\n",
    "- 4, 4, 3, 2\n",
    "- 4, 3, 2 ,1 \n",
    "- **<font color='yellow'>[x] 4, 4, 3, 2, 1</font>**\n",
    "\n",
    "  **<font color='green'> The $n^{[l]}$ are the number of units in each layer, notice that $n^{[0]} = n_x$..</font>** \n",
    "\n",
    "- 4, 3, 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7\n",
    "\n",
    "During forward propagation, in the forward function for a layer l you need to know what is the activation function in a layer (Sigmoid, tanh, ReLU, etc.). During backpropagation, the corresponding backward function also needs to know what is the activation function for layer l, since the gradient depends on it. True/False?\n",
    "\n",
    "- [ ] False\n",
    "\n",
    "- **<font color='yellow'>[x]True</font>**\n",
    "\n",
    "  **<font color='green'> During backpropagation you need to know which activation was used in the forward propagation to be able to compute the correct derivative.</font>**     \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8\n",
    "\n",
    "There are certain functions with the following properties:\n",
    "\n",
    "(i) To compute the function using a shallow network circuit, you will need a large network (where we measure size by the number of logic gates in the network), but (ii) To compute it using a deep network circuit, you need only an exponentially smaller network. True/False?\n",
    "    \n",
    "- **<font color='yellow'>[x]True</font>**\n",
    "\n",
    "- [ ] False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 9\n",
    "\n",
    "Consider the following 2 hidden layer neural network:\n",
    "\n",
    "<figure>\n",
    " <img src=\"./W42.png\"   style=\"width:800px;height:400px;\">\n",
    "</figure>\n",
    "\n",
    "Which of the following statements are True? (Check all that apply).\n",
    "\n",
    "- $b^{[1]}$ will have shape (3, 1)\n",
    "\n",
    "- $W^{[3]}$ will have shape (3, 1)\n",
    "\n",
    "- **<font color='yellow'>[x] $W^{[2]}$ will have shape (3, 4)</font>**\n",
    "  \n",
    "  **<font color='green'> More generally, the shape of $W^{[1]}$ is $(n^{[1]}, n^{[n-1]})$.</font>**\n",
    "\n",
    "- $b^{[3]}$ will have shape (3, 1)\n",
    "\n",
    "- **<font color='yellow'>[x] $ b^{[1]}$ will have shape (4, 1)</font>**\n",
    "\n",
    "  **<font color='green'> More generally, the shape of $b^{[1]}$ is $(n^{[1]}, 1)$.</font>**\n",
    "\n",
    "- $W^{[1]}$ will have shape (3, 4)\n",
    "\n",
    "- $W^{[2]}$ will have shape (3, 1)  \n",
    "\n",
    "- **<font color='yellow'>[x] $W^{[3]}$ will have shape (1, 3)</font>**\n",
    "  \n",
    "  **<font color='green'> More generally, the shape of $W^{[1]}$ is $(n^{[1]}, n^{[n-1]})$.</font>**\n",
    "\n",
    "- $b^{[2]}$ will have shape (1, 1)  \n",
    "\n",
    "- **<font color='yellow'>[x] $W^{[1]}$ will have shape (4, 4)</font>**\n",
    "  \n",
    "  **<font color='green'> More generally, the shape of $W^{[1]}$ is $(n^{[1]}, n^{[n-1]})$.</font>**\n",
    "\n",
    "- **<font color='yellow'>[x] $b^{[2]}$ will have shape (3, 1)</font>**\n",
    "  \n",
    "  **<font color='green'> More generally, the shape of $b^{[1]}$ is $(n^{[1]}, 1)$.</font>** \n",
    "\n",
    "\n",
    "- **<font color='yellow'>[x] $b^{[3]}$ will have shape (1, 1)</font>**\n",
    "  \n",
    "  **<font color='green'> More generally, the shape of $b^{[1]}$ is $(n^{[1]}, 1)$.</font>** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 10 \n",
    "\n",
    "In the general case if we are training with $m$ examples \n",
    "\n",
    "What is the shape of $A^{[l]}$ ?\n",
    "\n",
    "- $(n^{[l+1]}, m)$\n",
    "\n",
    "- $(m, n^{[l]})$\n",
    "\n",
    "- **<font color='yellow'>[x] $(n^{[l]}, n)$</font>**\n",
    "\n",
    "  **<font color='green'> The number of rows in $A^{[1]}$ corresponds to the number of units in the l-th layer.</font>** \n",
    "\n",
    "- $(m, n^{[l+1]})$\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
