{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "Which of the following are true? (Check all that apply.)\n",
    "\n",
    "- $a_{4}^{[2]}$ is the activation output of the $2^{\\text{nd}}$ layer for the $4^{\\text{th}}$ training example\n",
    "\n",
    "- $a^{[2]}$ denotes the activation vector of the $2^{\\text{nd}}$ layer\n",
    "\n",
    "- $a^{[2](12)}$ denotes the activation vector of the $2^{\\text{nd}}$ layer for the $12^{\\text{th}}$ training example\n",
    "\n",
    "- $X$ is a matrix in which each row is one training example\n",
    "\n",
    "- $a^{[2](12)}$ denotes activation vector of the $12^{\\text{nd}}$ layer on the $2^{\\text{nd}}$ layer\n",
    "\n",
    "- $a_{4}^{[2]}$ is the activation output by  the $4^{\\text{th}}$ neuron of the $2^{\\text{nd}}$ layer\n",
    "\n",
    "- $X$ is a matrix in which each column is one training example\n",
    "\n",
    "**<font color='yellow'>Answer: (1) $a^{[2]}$ denotes the activation vector of the $2^{\\text{nd}}$ layer (2) $a^{[2](12)}$ denotes the activation vector of the $2^{\\text{nd}}$ layer for the $12^{\\text{th}}$ training example (3) $a_{4}^{[2]}$ is the activation output by  the $4^{\\text{th}}$ neuron of the $2^{\\text{nd}}$ layer (4) $X$ is a matrix in which each column is one training example</font>** \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "In which of the following cases is the linear (identity) activation function most likely used?\n",
    "\n",
    "- For binary classification problems\n",
    "\n",
    "- When working with regression problems\n",
    "\n",
    "- The linear activation function is never used\n",
    "\n",
    "- As activation function in the hidden layers\n",
    "\n",
    "**<font color='yellow'>Answer: When working with regression problems.</font>** \n",
    "\n",
    "**<font color='yellow'>Explanation: In problems such as predicting the price of a house it makes sense to use the linear activation function as output. </font>** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "Which of the following represents the activation output of the second neuron of the third layer applied to the fourth example?\n",
    "\n",
    "<figure>\n",
    " <img src=\"./W3.png\"   style=\"width:800px;height:400px;\">\n",
    "</figure>\n",
    "\n",
    "- $a_{2}^{[3](4)}$\n",
    "\n",
    "- $a_{2}^{[4](3)}$\n",
    "\n",
    "- $a_{4}^{[3](2)}$\n",
    "\n",
    "- $a_{3}^{[4]2}$\n",
    "\n",
    "**<font color='yellow'>Answer: $a_{2}^{[3](4)}$.</font>** \n",
    "\n",
    "**<font color='yellow'>Explanation: The superscript in brackets indicates the layer number, the superscript in parenthesis represents the number of examples, and the subscript the number of the neuron. </font>** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "\n",
    "The use of the ReLU activation function is becoming more rare because the ReLU function has no derivative for $c = 0$. True/False?\n",
    "\n",
    "- True\n",
    "\n",
    "- False\n",
    "\n",
    "\n",
    "**<font color='yellow'>Answer: False.</font>** \n",
    "\n",
    "**<font color='yellow'>Explanation: Although the ReLU function has no derivative at $$c=0$$ this rarely causes any problems in practice. Moreover it has become the default activation function in many cases, as explained in the lectures.. </font>** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "\n",
    "Consider the following code:\n",
    "\n",
    "*#+begin_src python*\n",
    "\n",
    "*x = np.random.rand(4, 5)*\n",
    "\n",
    "*y = np.sum(x, axis=1)*\n",
    "\n",
    "*#+end_src*\n",
    "\n",
    "What will be y.shape?\n",
    "\n",
    "- (4,1)\n",
    "\n",
    "- (5,)\n",
    "\n",
    "- (4,)\n",
    "\n",
    "- (1,5)\n",
    "\n",
    "\n",
    "\n",
    "**<font color='yellow'>Answer: .</font>** \n",
    "\n",
    "**<font color='yellow'>Explanation: . </font>** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6\n",
    "\n",
    "Suppose you have built a neural network with one hidden layer and tanh as activation function for the hidden layer. You decide to initialize the weights to small random numbers and the biases to zero. The first hidden layer’s neurons will perform different computations from each other even in the first iteration. True/False?\n",
    "\n",
    "\n",
    "- False No. Since the weights are most likely different, each neuron will do a different computation.\n",
    "\n",
    "- True Yes. Since the weights are most likely different, each neuron will do a different\n",
    "\n",
    "\n",
    "**<font color='yellow'>Answer: True Yes. Since the weights are most likely different, each neuron will do a different.</font>** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7\n",
    "\n",
    "A single output and single layer neural network that uses the sigmoid function as activation is equivalent to the logistic regression. True/False\n",
    "\n",
    "\n",
    "- True\n",
    "\n",
    "- False\n",
    "\n",
    "\n",
    "**<font color='yellow'>Answer: True.</font>** \n",
    "\n",
    "**<font color='yellow'>Explanation: The logistic regression model can be expressed by $$ŷ = σ\\left( W \\, x + b \\right)$$ This is the same as $$a^{[1]} = σ( W^{[1]}\\, X + b )$$</font>** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8\n",
    "\n",
    "You have built a network using the tanh activation for all the hidden units. You initialize the weights to relatively large values, using np.random.randn(..,..)*1000. What will happen?\n",
    "\n",
    "\n",
    "- This will cause the inputs of the tanh to also vey large, thus causing gradients to also become large. You therefore have to set $\\alpha$ to a very small value to prevent divergence; this will slow down learning.\n",
    "\n",
    "- This will cause the inputs of the tanh to also be very large, causing the units to be “highly activated” and thus speed up learning compared to if the weights had to start from small values.\n",
    "\n",
    "- This will cause the inputs of the tanh to also be very large, thus causing gradients to be close to zero. The optimization algorithm will thus become slow.\n",
    "\n",
    "- It doesn’t matter. So long as you initialize the weights randomly gradient descent is not affected by whether the weights are large or small.\n",
    "\n",
    "\n",
    "**<font color='yellow'>Answer: This will cause the inputs of the tanh to also be very large, thus causing gradients to be close to zero. The optimization algorithm will thus become slow.</font>** \n",
    "\n",
    "**<font color='yellow'>Explanation: tanh becomes flat for large values; this leads its gradient to be close to zero. This slows down the optimization algorithm.</font>** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 9\n",
    "\n",
    "Consider the following 1 hidden layer neural network:\n",
    "\n",
    "<figure>\n",
    " <img src=\"./W32.png\"   style=\"width:800px;height:400px;\">\n",
    "</figure>\n",
    "\n",
    "Which of the following statements are True? (Check all that apply).\n",
    "\n",
    "- $W^{[1]}$ will have shape (4,3)\n",
    "\n",
    "- $b_{[1]}$ will have shape (3,1)\n",
    "\n",
    "- $W^{[1]}$ will have shape (3,4)\n",
    "\n",
    "- E: $b^{[2]}$ will have shape (1,1)\n",
    "\n",
    "**<font color='yellow'>Answer: (1) $b_{[1]}$ will have shape (3,1) (2) $W^{[1]}$ will have shape (3,4) (3) E: $b^{[2]}$ will have shape (1,1).</font>** \n",
    "\n",
    "**<font color='yellow'>Explanation: (1) $b_{[k]}$ is a column vector and has the same number of rows as neurons in the k-th layer (2) The number of rows in $W^{[k]}$ is the number of neurons in the k-th layer and the number of columns is the number of inputs of the layer. </font>** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 10\n",
    "\n",
    "Consider the following 1 hidden layer neural network:\n",
    "\n",
    "<figure>\n",
    " <img src=\"./W33.png\"   style=\"width:800px;height:400px;\">\n",
    "</figure>\n",
    "\n",
    "What are the dimensions of $Z^{[1]}$ and $A^{[1]}$?\n",
    "\n",
    "- $Z^{[1]}$ and $A^{[1]}$ are (4,m)\n",
    "\n",
    "- $Z^{[1]}$ and $A^{[1]}$ are (4,1)\n",
    "\n",
    "- $Z^{[1]}$ and $A^{[1]}$ are (2,1)\n",
    "\n",
    "- $Z^{[1]}$ and $A^{[1]}$ are (2,m)\n",
    "\n",
    "**<font color='yellow'>Answer: $Z^{[1]}$ and $A^{[1]}$ are (2,m).</font>** \n",
    "\n",
    "**<font color='yellow'>Explanation: The $Z^{[1]}$ and $A^{[1]}$ are calculated over a batch of training examples. The number of columns in $Z^{[1]}$ and $A^{[1]}$ is equal to the number of examples in the batch, m. And the number of rows in $Z^{[1]}$ and $A^{[1]}$ is equal to the number of neurons in the first layer. </font>** \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
